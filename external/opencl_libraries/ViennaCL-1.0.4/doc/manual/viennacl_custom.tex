

\chapter{Custom Compute Kernels} \label{chap:custom}

For custom algorithms the built-in functionality of {\ViennaCL} may not be sufficient or not fast enough. In such cases it can be desireable to write a custom {\OpenCL} compute kernel, which is explained in this chapter. The following steps are necessary and explained one after another:
\begin{itemize}
 \item Write the {\OpenCL} source code
 \item Compile the compute kernel
 \item Launch the kernel
\end{itemize}
A tutorial on this topic can be found at \texttt{examples/tutorial/tut5.cpp}.

\section{Setting up the Source Code}
The {\OpenCL} source code has to be provided as a string. You can either write the source code directly into a string in you C++ files, or you can read the {\OpenCL} source from a file. For demonstration purposes, we write the source directly as a string constant:
\begin{lstlisting}
const char * my_compute_kernel = 
"__kernel void elementwise_prod(\n"
"          __global const float * vec1,\n"
"          __global const float * vec2, \n"
"          __global float * result,\n"
"          unsigned int size) \n"
"{ \n"
"  for (unsigned int i = get_global_id(0); i < size; i += get_global_size(0))\n"
"    result[i] = vec1[i] * vec2[i];\n"
"};\n";
\end{lstlisting}
The kernel takes three vector arguments \lstinline{vec1}, \lstinline{vec2} and \lstinline{result} and the vector length variable \lstinline{size}. The compute kernel computes the entry-wise product of the vectors \lstinline|vec1| and \lstinline|vec2| and writes the result to the vector \lstinline{vec3}. For more detailed explanation of the {\OpenCL} source code, please refer to the specification available at the Khronos group webpage \cite{khronoscl}.

\section{Compilation of the Source Code}
The source code in the string constant \lstinline{my_compute_kernel} has to be compiled to an {\OpenCL} program. An {\OpenCL} program is a compilation unit and may contain several different compute kernels, so one could also include another kernel function \lstinline{inplace_elementwise_prod} which writes the result directly to one of the two operands \lstinline{vec1} or \lstinline{vec2} in the same program.
\begin{lstlisting}
  viennacl::ocl::program my_prog;        //create the object
  my_prog.create(my_compute_kernel);     //compile the source
\end{lstlisting}
The next step is to extract the kernel \lstinline|my_compute_kernel| from the compiled program:
\begin{lstlisting}
  viennacl::ocl::kernel my_kernel;                 //create kernel object
  my_kernel.prepareInit("elementwise_prod", my_prog);  //extract kernel
\end{lstlisting}
Now, the kernel is set up to use the function \lstinline|elementwise\_prod| compiled into the program \lstinline|my\_prog|.

\TIP{The kernel extraction actually occurs at the time the first kernel argument is set. This allows to avoid any kernel extraction overhead for unused kernels.}

\NOTE{Due to the just-in-time extraction, kernel objects cannot be copied or assigned to other kernel objects.}

\section{Launch the Kernel}
To launch the kernel, the kernel arguments have to be set. We assume that three {\ViennaCL} vectors \lstinline|vec1|, \lstinline|vec2| and \lstinline|result| have already been set up, the vector length is available in the variable \lstinline|vector_size|:
\begin{lstlisting}
  unsigned int pos = 0;
  my_kernel.setArgument(pos++, vec1.handle());
  my_kernel.setArgument(pos++, vec2.handle());
  my_kernel.setArgument(pos++, result.handle());
  my_kernel.setArgument(pos++, vector_size);
\end{lstlisting}
The {\OpenCL} handles for the vectors are obtained by the member function \lstinline{handle()}, the vector length argument can be passed directly. The kernel is now ready for launch:
\begin{lstlisting}
  my_kernel.start1D(vector_size,    //number of global threads
                    vector_size);   //number of threads per work group
\end{lstlisting}
The first argument specifies the number of global threads. Assuming small vectors (less than 100 entries), we can assign one vector per entry. The second argument specifies the number of threads per work group and can be omitted, in which case the {\OpenCL} driver tries to find suitable work group sizes automatically. Please consult the {\OpenCL} specification \cite{khronoscl} for more details on the execution model.

