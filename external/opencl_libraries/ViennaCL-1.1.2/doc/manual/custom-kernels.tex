

\chapter{Custom Compute Kernels} \label{chap:custom}

For custom algorithms the built-in functionality of {\ViennaCL} may not be sufficient or not fast enough. In such cases it can be desirable to write a custom {\OpenCL} compute kernel, which is explained in this chapter. The following steps are necessary and explained one after another:
\begin{itemize}
 \item Write the {\OpenCL} source code
 \item Compile the compute kernel
 \item Launching the kernel
\end{itemize}
A tutorial on this topic can be found at \texttt{examples/tutorial/custom-kernels.cpp}.

\TIP{The interface for custom kernels was simplified considerably in {\ViennaCL} 1.1.0.}

\section{Setting up the Source Code}
The {\OpenCL} source code has to be provided as a string. One can either write the source code directly into a string within C++ files, or one can read the {\OpenCL} source from a file. For demonstration purposes, we write the source directly as a string constant:
\begin{lstlisting}
const char * my_compute_kernel = 
"__kernel void elementwise_prod(\n"
"          __global const float * vec1,\n"
"          __global const float * vec2, \n"
"          __global float * result,\n"
"          unsigned int size) \n"
"{ \n"
"  for (unsigned int i = get_global_id(0); i < size; i += get_global_size(0))\n"
"    result[i] = vec1[i] * vec2[i];\n"
"};\n";
\end{lstlisting}
The kernel takes three vector arguments \lstinline{vec1}, \lstinline{vec2} and \lstinline{result} and the vector length variable \lstinline{size}. The compute kernel computes the entry-wise product of the vectors \lstinline|vec1| and \lstinline|vec2| and writes the result to the vector \lstinline|result|. For more detailed explanation of the {\OpenCL} source code, please refer to the specification available at the Khronos group webpage \cite{khronoscl}.

\section{Compilation of the Source Code}
The source code in the string constant \lstinline{my_compute_kernel} has to be compiled to an {\OpenCL} program.
An {\OpenCL} program is a compilation unit and may contain several different compute kernels,
so one could also include another kernel function \lstinline{inplace_elementwise_prod} which writes the result directly to one of the two operands \lstinline{vec1} or \lstinline{vec2} in the same program.
\begin{lstlisting}
viennacl::ocl::program & prog = 
  viennacl::ocl::current_context().add_program(my_compute_program,
                                               "my_compute_program");
\end{lstlisting}
The next step is to extract the kernel \lstinline|my_compute_kernel| from the compiled program:
\begin{lstlisting}
viennacl::ocl::kernel & my_kernel = my_prog.add_kernel("elementwise_prod");
\end{lstlisting}
Now, the kernel is set up to use the function \lstinline|elementwise_prod| compiled into the program \lstinline|my_prog|.

\NOTE{Note that C++ references to kernels and programs may become invalid as other kernels or programs are added. This is the case at the first instantiation of an {\ViennaCL} object of a particular type. Therefore, first allocate the required {\ViennaCL} objects and compile/add all custom kernels, before you start passing references to programs or kernels around.}

Instead of holding references to programs and kernels directly at compilation, one can obtain them at other places within the application source code by
\begin{lstlisting}
viennacl::ocl::program & prog = 
  viennacl::ocl::current_context().get_program("my_compute_program");
viennacl::ocl::kernel & my_kernel = my_prog.get_kernel("elementwise_prod");
\end{lstlisting}

\section{Launching the Kernel}
Before launching the kernel, one may adjust the global and local work sizes (readers not familiar with that are encouraged to read the {\OpenCL} standard \cite{khronoscl}).
The following code specifies a one-dimensional execution model with 16 local workers and 128 global workers:
\begin{lstlisting}
 my_kernel.local_work_size(0, 16);
 my_kernel.global_work_size(0, 128);
\end{lstlisting}
In order to use a two-dimensional execution, additionally parameters for the second dimension are set by
\begin{lstlisting}
 my_kernel.local_work_size(1, 16);
 my_kernel.global_work_size(1, 128);
\end{lstlisting}
However, for the simple kernel in this example it is not necessary to specify any work sizes at all. The default work sizes (which can be found in \texttt{viennacl/ocl/kernel.hpp}) suffice for most cases.

To launch the kernel, the kernel arguments are set in the same way as for ordinary functions. We assume that three {\ViennaCL} vectors \lstinline|vec1|, \lstinline|vec2| and \lstinline|result| have already been set up:
\begin{lstlisting}
  viennacl::ocl::enqueue(my_kernel(vec1, vec2, result, vec1.size()));
\end{lstlisting}
Per default, the kernel is enqueued in the first queue of the currently active device. A custom queue can be specified as optional second argument, cf.~the reference documentation
located in \texttt{doc/doxygen/}.
