\chapter{Basic Types}
This chapter provides a brief overview of the basic interfaces and usage of the
provided data types. The term \textit{GPU} refers here and in the following to
both GPUs and multi-core CPUs accessed via {\OpenCL} and managed by
{\ViennaCL}. Operations on the various types are explained in
Chap.~\ref{chap:operations}. For full details, refer to the reference pages
in the folder
\texttt{doc/doxygen}.

\section {Scalar Type}
The scalar type \lstinline|scalar<T>| with template parameter T
denoting the underlying CPU scalar type (float and double, if supported - see Tab.~\ref{tab:double-precision-GPUs}) represents a
single scalar value on the GPU. \lstinline|scalar<T>| is designed to behave much
like a scalar type on the CPU, but library users have to keep in mind that
every operation on \lstinline|scalar<T>| requires to launch the appropriate
compute kernel on the GPU and is thus much slower then the CPU equivalent.

\NOTE{Be aware that operations between objects of type \lstinline|scalar<T>|
(e.g.~additions. comparisons) have large overhead. For every operation, a
separate compute kernel launch is required.}

\subsection{Example Usage}
The scalar type of {\ViennaCL} can be used just like the built-in
types, as the following snippet shows:
\begin{lstlisting}
  float cpu_float = 42.0f;
  double cpu_double = 13.7603;
  ViennaCL::scalar<float>  gpu_float(3.1415f);
  ViennaCL::scalar<double> gpu_double = 2.71828;

  //conversions and t
  cpu_float = gpu_float;  
  gpu_float = cpu_double;  //automatic transfer and conversion

  cpu_float = gpu_float * 2.0f; 
  cpu_double = gpu_float - cpu_float;
\end{lstlisting}
Mixing built-in types with the {\ViennaCL} scalar is usually not a
problem. Nevertheless, since every operation requires {\OpenCL} calls, such
arithmetics should be used sparsingly.

\NOTE{In the present version of {\ViennaCL}, it is not possible to assign a \lstinline|scalar<float>| to a \lstinline|scalar<double>| directly.}

\subsection{Members}
Apart from suitably overloaded operators that mimic the behavior of the
respective CPU counterparts, only a single public member function
\lstinline|handle()| is available, cf.~Tab.~\ref{tab:scalar-interface}.

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8.5cm}}
Interface & Comment\\
\hline
\texttt{v.handle()}   & The GPU handle \\
\end{tabular}
\caption{Interface of \texttt{vector$<$T$>$} in \ViennaCL. Destructors and
operator overloads for BLAS are not listed.}
\label{tab:scalar-interface}
\end{center}
\end{table}



\section{Vector Type}
The main vector type in {\ViennaCL} is \texttt{vector$<$T, alignment$>$},
representing a chunk of memory on the compute device. \texttt{T} is the
underlying scalar type (either \texttt{float} or \texttt{double} if supported, cf.~Tab.~\ref{tab:double-precision-GPUs}, complex types
are not supported in \ViennaCLversion) and the optional argument \texttt{alignment} denotes the memory
the vector is aligned to (in multiples of \texttt{sizeof(T)}). For example, a
vector with a size of 55 entries and an alignment of 16 will reside in a
block of memory equal to 64 entries. Memory alignment is fully
transparent, so from the end-user's point of view, \texttt{alignment} allows to
tune {\ViennaCL} for maximum speed on the available compute device.

At construction, \texttt{vector$<$T, alignment$>$} is initialized to have the
supplied length, but the memory is not initialized to zero. Another difference
to CPU implementations is that accessing single vector elements is very costly,
because every time an element is
accessed, it has to be transferred from the CPU to the compute device or vice
versa. 
\subsection{Example Usage}
The following code snippet shows the typical use of the vector type provided by
{\ViennaCL}. The overloaded function \texttt{copy()} function, which is used as
in the STL, should be used for the initialization of vector entries:
\begin{lstlisting}
std::vector<ScalarType>      stl_vec(10);
viennacl::vector<ScalarType> vcl_vec(10);

//fill the STL vector:
for (unsigned int i=0; i<vector_size; ++i)
  stl_vec[i] = i;

//copy content to GPU vector (recommended initialization)
copy(stl_vec.begin(), stl_vec.end(), vcl_vec.begin());

//manipulate GPU vector here

//copy content from GPU vector back to STL vector
copy(vcl_vec.begin(), vcl_vec.end(), stl_vec.begin());
\end{lstlisting}

\TIP{The function copy() does not assume that the values of the supplied CPU
object are located in a linear memory sequence. If this is the case, the
function \lstinline{fast_copy} provides better performance.}

Once the vectors are set up on the GPU, they can be used like objects on the CPU
(cf.~Chap.~\ref{chap:operations}):
\begin{lstlisting}
// let vcl_vec1 and vcl_vec2 denote two vector on the GPU
vcl_vec1 *= 2.0;
vcl_vec2 += vcl_vec1;
vcl_vec1 = vcl_vec1 - 3.0 * vcl_vec2;
\end{lstlisting}

\subsection{Members}
At construction, \texttt{vector$<$T, alignment$>$} is initialized to have the
supplied length, but memory is not initialized. If initialization is desired,
the memory can be initialized with zero values using the member function
\lstinline|clear()|. See
Tab.~\ref{tab:vector-interface} for other member functions.

\NOTE{Accessing single elements of a vector using operator() or operator[] is
very slow! Use with care!}

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8.5cm}}
Interface & Comment\\
\hline
\texttt{CTOR(n)}    & Constructor with number of entries \\
\texttt{v(i)}    & Access to the $i$-th element of v (slow!) \\
\texttt{v[i]}    & Access to the $i$-th element of v (slow!) \\
\texttt{v.clear()}    & Initialize v with zeros \\
\texttt{v.resize(n, bool preserve)}    & Resize v to length n. Preserves old values if bool is true. \\
\texttt{v.begin()}   & Iterator to the begin of the matrix \\
\texttt{v.end()}   & Iterator to the end of the matrix \\
\texttt{v.size()}    & Length of the vector \\
\texttt{v.swap(v2)}   & Swap the content of v with v2 \\
\texttt{v.internal\_size()} & Returns the number of entries allocated on the GPU (taking alignment into account) \\
\texttt{v.empty()}   & Shorthand notation for \texttt{v.size() == 0} \\
\texttt{v.clear()}   & Sets all entries in v to zero \\
\texttt{v.handle()}  & Returns the GPU handle (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of \texttt{vector$<$T$>$} in \ViennaCL. Destructors and
operator overloads for BLAS are not listed.}
\label{tab:vector-interface}
\end{center}
\end{table}

One important difference to CPU implementations is that the bracket operator
(as well as the parenthesis operator) is very slow, because for each access on the GPU a
data transfer has to be initiated. The overhead of this transfer is orders of
magnitude. For example:

  \begin{lstlisting}
// fill a vector on CPU
for (long i=0; i<cpu_vector.size(); ++i)
    cpu_vector(i) = 1e-3f;

// fill a vector on GPU - VERY SLOW!!
for (long i=0; i<gpu_vector.size(); ++i)
    gpu_vector(i) = 1e-3f;
\end{lstlisting} 
The difference in execution speed is typically several orders of magnitude,
therefore direct vector element access should be used only if a very small
number of entries is accessed in this way. A much faster initialization is as
follows:

  \begin{lstlisting}
// fill a vector on CPU
for (long i=0; i<cpu_vector.size(); ++i)
    cpu_vector(i) = 1e-3f;

// fill a vector on GPU with data from CPU - fast version
copy(cpu_vector, gpu_vector);
\end{lstlisting} 
In this way, setup costs for the CPU vector and the GPU vector are comparable.

\section{Dense Matrix Type}
\texttt{matrix$<$T, F, alignment$>$} represents a dense matrix with interface listed in
Tab.~\ref{tab:matrix-interface}. The second optional template argument \texttt{F}
specifies the storage layout and defaults to \texttt{row\_major}, which is the only one provided in {\ViennaCLminorversion}.
The third template argument \texttt{alignment} denotes an alignemnt for the number of rows (cf.~\texttt{alignment} for the \texttt{vector} type).

\subsection{Example Usage}
The use of \texttt{matrix$<$T, F$>$} is similar to that of the counterpart in {\ublas}. The operators are overloaded similarly.

\begin{lstlisting}
 //set up a 3 by 5 matrix:
 viennacl::matrix<float>  gpu_matrix(3, 5);

 //fill it up:
 gpu_matrix(0,2) = 1.0; 
 gpu_matrix(1,2) = -1.5; 
 gpu_matrix(3,0) = 4.2; 
\end{lstlisting} 

\NOTE{Accessing single elements of a matrix using \texttt{operator()} is very slow! Use with care!}

A much better way is to initialize a dense matrix using the provided \texttt{copy()} function:
\begin{lstlisting}
//copy content from CPU matrix to GPU matrix
copy(cpu_matrix, gpu_matrix);

//copy content from GPU matrix to CPU matrix
copy(gpu_matrix, cpu_matrix);
\end{lstlisting} 
The type requirement on the \texttt{cpu\_matrix} is that \texttt{operator()} can be used for accessing entries, that a member function \texttt{size1()} returns the number of rows and that \texttt{size2()} returns the number of columns.

\subsection{Members}

The members are listed in Tab.~\ref{tab:matrix-interface}. The usual operator overloads are not listed explicitly

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8.5cm}}
Interface & Comment\\
\hline
\texttt{CTOR(nrows, ncols)}    & Constructor with number of rows and columns \\
\texttt{mat(i,j)}    & Access to the element in the $i$-th row and the $j$-th column of mat \\
%\texttt{mat.clear()}    & Initialize mat with zeros \\
\parbox{6cm}{\texttt{mat.resize(m, n, \\
           \hphantom{mat.resize(}bool preserve)}}    & Resize mat to m rows and n columns. Currently, the boolean flag is ignored and entries always discarded. \\
%\texttt{mat.begin()}   & Iterator to the begin of the matrix \\
%\texttt{mat.end()}   & Iterator to the end of the matrix \\
\texttt{mat.size1()}            & Number of rows in mat \\
\texttt{mat.internal\_size1()}   & Internal number of rows in mat \\
\texttt{mat.size2()}            & Number of columns in mat \\
\texttt{mat.internal\_size2()}   & Internal number of columns in mat \\
\texttt{mat.clear()}   & Sets all entries in v to zero \\
\texttt{mat.handle()}  & Returns the GPU handle (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of the dense matrix type \texttt{matrix$<$T, F$>$} in
\ViennaCL. Constructors, Destructors and operator overloads for BLAS are not
listed.}
\label{tab:matrix-interface}
\end{center}
\end{table}



\section{Sparse Matrix Types}

There are two different sparse matrix types provided in {\ViennaCL}, \texttt{compressed\_matrix} and \texttt{coordinate\_matrix}.

\TIP{In {\ViennaCLversion}, the use of \texttt{compressed\_matrix} is encouraged over \texttt{coordinate\_matrix}}

\subsection{Compressed Matrix}
\texttt{compressed\_matrix$<$T, alignment$>$}, represents a sparse
matrix using a compressed sparse row scheme. Again, \texttt{T} is the floating point type. \texttt{alignment} is the alignment and defaults to \texttt{1} at present.
In general, sparse matrices should be set up on the
CPU and then be pushed to the compute device using \texttt{copy()}, because the management of map
based sparse matrices is inefficient on most compute devices such as GPUs.

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8cm}}
Interface & Comment\\
\hline
\texttt{CTOR(nrows, ncols)}    & Constructor with number of rows and columns \\
%\texttt{mat(i,j) const}    & Read-only access to the element in the $i$-th row and the $j$-th column of mat \\
%\texttt{mat.readFrom(PointerType prows, PointerType pcols, PointerType pentries)} & Fill mat with the values from the supplied piece of memory. \\
%\texttt{mat.writeTo(PointerType prows, PointerType pcols, PointerType pentries)}  & Fill the supplied piece of memory with values from mat. \\ \\
\texttt{mat.set()}    & Initialize mat with zeros \\
\texttt{mat.reserve(num)}    & Reserve memory for up to \texttt{num} nonzero entries \\
%\texttt{mat.clear()}    & Initialize mat with zeros \\

\texttt{mat.size1()}            & Number of rows in mat \\
\texttt{mat.size2()}            & Number of columns in mat \\
\texttt{mat.nnz()}		& Number of nonzeroes in mat \\
\parbox{6cm}{\texttt{mat.resize(m, n, \\
           \hphantom{mat.resize(}bool preserve)}}    & Resize mat to m rows and n columns. Currently, the boolean flag is ignored and entries always discarded. \\
\texttt{mat.handle1()}  & Returns the GPU handle holding the row indices (needed for custom kernels, see Chap.~\ref{chap:custom}) \\
\texttt{mat.handle2()}  & Returns the GPU handle holding the column indices  (needed for custom kernels, see Chap.~\ref{chap:custom}) \\
\texttt{mat.handle()}  & Returns the GPU handle holding the entries (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of the sparse matrix type \texttt{compressed\_matrix$<$T, F$>$} in \ViennaCL. Destructors and operator overloads for BLAS are not listed.}
\label{tab:compressed-matrix-interface}
\end{center}
\end{table}

\subsubsection{Example Usage} \label{sec:compressed-matrix-example}
The use of \texttt{compressed\_matrix$<$T, alignment$>$} is similar to that of the counterpart in {\ublas}. The operators are overloaded similarly.
There is a direct interfacing with the standard implementation using a vector of maps from the STL:
\begin{lstlisting}
 //set up a sparse 3 by 5 matrix on the CPU:
 std::vector< std::map< unsigned int, float> > cpu_sparse_matrix(3);

 //fill it up:
 cpu_sparse_matrix[0][2] = 1.0; 
 cpu_sparse_matrix[1][2] = -1.5; 
 cpu_sparse_matrix[3][0] = 4.2; 

 //set up a sparse GPU matrix:
 viennacl::compressed_matrix<float>  gpu_sparse_matrix(3, 5);

 //copy to GPU:
 copy(cpu_sparse_matrix, gpu_sparse_matrix);

 //copy back to CPU:
 copy(gpu_sparse_matrix, cpu_sparse_matrix);
\end{lstlisting}
The \texttt{copy()} functions can also be used with a generic sparse matrix data type fulfilling the following requirements:
\begin{itemize}
 \item The \texttt{const\_iterator1} type is provided for iteration along increasing row index
 \item The \texttt{const\_iterator2} type is provided for iteration along increasing column index
 \item \texttt{.begin1()} returns an iterator pointing to the element with indices $(0,0)$.
 \item \texttt{.end1()} returns an iterator pointing to the end of the first column
 \item When copying to the cpu type: Write operation via \texttt{operator()}
 \item When copying to the cpu type: \texttt{resize(m,n,preserve)} member (cf.~Tab.~\ref{tab:compressed-matrix-interface})
\end{itemize}
The iterator returned from the cpu sparse matrix type via \texttt{begin1()} has to fulfill the following requirements:
\begin{itemize}
 \item \texttt{.begin()} returns an column iterator pointing to the first nonzero element in the particular row.
 \item \texttt{.end()} returns an iterator pointing to the end of the row
 \item Increment and dereference
\end{itemize}
For the sparse matrix types in {\ublas}, these requirements are all fulfilled.

\subsubsection{Members}
The interface is described in Tab.~\ref{tab:compressed-matrix-interface}. 

\subsection{Coordinate Matrix}
In the second sparse matrix type, \texttt{coordinate\_matrix$<$T, alignment$>$}, 
entries are stored as triplets \texttt{(i,j,val)}, where \texttt{i} is the row index, \texttt{j} is the column index and \texttt{val} is the entry. 
Again, \texttt{T} is the floating point type. The optional \texttt{alignment} defaults to \texttt{1} at present.
In general, sparse matrices should be set up on the
CPU and then be pushed to the compute device using \texttt{copy()}, because the management of map
based sparse matrices is inefficient on most compute devices such as GPUs.

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8cm}}
Interface & Comment\\
\hline
\texttt{CTOR(nrows, ncols)}    & Constructor with number of rows and columns \\
%\texttt{mat(i,j) const}    & Read-only access to the element in the $i$-th row and the $j$-th column of mat \\
%\texttt{mat.readFrom(PointerType prows, PointerType pcols, PointerType pentries)} & Fill mat with the values from the supplied piece of memory. \\
%\texttt{mat.writeTo(PointerType prows, PointerType pcols, PointerType pentries)}  & Fill the supplied piece of memory with values from mat. \\ \\
\texttt{mat.reserve(num)}    & Reserve memory for up to \texttt{num} nonzero entries \\
\texttt{mat.size1()}            & Number of rows in mat \\
\texttt{mat.size2()}            & Number of columns in mat \\
\texttt{mat.nnz()}		& Number of nonzeroes in mat \\
\parbox{6cm}{\texttt{mat.resize(m, n, \\
           \hphantom{mat.resize(}bool preserve)}}    & Resize mat to m rows and n columns. Currently, the boolean flag is ignored and entries always discarded. \\
%\texttt{mat.clear()}    & Initialize mat with zeros \\
\texttt{mat.resize(m, n)}    & Resize mat to m rows and n columns. Does not preserve old values. \\
\texttt{mat.handle12()}  & Returns the GPU handle holding the row and column indices (needed for custom kernels, see Chap.~\ref{chap:custom}) \\
\texttt{mat.handle()}  & Returns the GPU handle holding the entries (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of the sparse matrix type \texttt{coordinate\_matrix$<$T, A$>$} in \ViennaCL. Destructors and operator overloads for BLAS are not listed.}
\label{tab:coordinate-matrix-interface}
\end{center}
\end{table}

\subsubsection{Example Usage}
The use of \texttt{coordinate\_matrix$<$T, alignment$>$} is similar to that of the first sparse matrix type 
\texttt{compressed\_matrix$<$T, alignment$>$}, thus we refer to Sec.~\ref{sec:compressed-matrix-example}


\subsubsection{Members}
The interface is described in Tab.~\ref{tab:coordinate-matrix-interface}. 
