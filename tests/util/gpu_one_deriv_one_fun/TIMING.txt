100x100x100, stencil:32, Random

I just realized: as I increase number of derivatives I am computing, cache
becomes scarcer.

4 derivatives of 1 function :   avg:   344.3620  |  tot:   200
4 derivatives of 2 functions:   avg:   344.3620  |  tot:   250
4 derivatives of 3 functions:   avg:   344.3620  |  tot:   344.3620 (speedup: 2)
4 derivatives of 4 functions:   avg:   391.3620   (factor 2 speedup over 4 derivatives of 1 function)
----------------------------------------------------------------------
I believe I can get additional speedup using double4. The key is returning data to regular memory. 

Another issue: what is the effect of compacting the data on the subsequent solution update. 
One must consider the cost of converting from one format to another. 

Using temporaries for weights did not change timings. 
----------------------------------------------------------------------
Using stencil:64, Random: program crashed. WHY? 
  Guess: allocating too much memory at once. 
  stencil: 64*10^6 = 64 million, perhaps *4 = 256 millon bytes (senticl indices
Using stencil:32, Random: 375ms
Using stencil:32, Compact: 240ms
----------------------------------------------------------------------
100x50x50, 
Using stencil:64, ST_RANDOM, 132ms (4 functions, 4 derivatives)
Using stencil:32, ST_RANDOM, 67ms (4 functions, 4 derivatives) (2x slower as expected)
Using stencil:16, ST_RANDOM, 35ms (4 functions, 4 derivatives) (2x slower as expected)
Using stencil: 8, ST_RANDOM, 20ms (4 functions, 4 derivatives) (2x slower as expected)
(linear scaling, when computing x,y,z,l derivatives of u,v,w,p
----------------------------------------------------------------------
Using floats instead of doubles: 
Using stencil: 8, ST_RANDOM, 20ms (4 functions, 4 derivatives) (2x slower as expected)
Using stencil: 64, ST_RANDOM, 125ms (4 functions, 4 derivatives) (2x slower as expected)
(stick with floats; more predictable and flexible.)
----------------------------------------------------------------------
No effect of worksize. One thread per stencil. 
Using stencil:64  tot:   127.1530ms, 32 items per workgroup. 
Using stencil:64  tot:   130.1530ms, 64 items per workgroup. 
Using stencil:64  tot:   134.1530ms, 128 items per workgroup. 

Using stencil:16  tot:   36.0ms, 8 items per workgroup. 
Using stencil:16  tot:   32.5ms, 32 items per workgroup. 
Using stencil:16  tot:   33ms, 64 items per workgroup. 
Using stencil:16  tot:   34.0ms, 128 items per workgroup. 

Nb items per workgroup is not affecting much. 8 items per workgroup is bad,
but the code is so inefficient, that this is not seen. 
----------------------------------------------------------------------
stencil: 32, RANDOM, 250,000 pts
GPU: 53.4420ms (no weights in derivative calculations) 
GPU: 80.4420ms (include weights in derv calculation)
(not much difference. Not enough to justify 2 kernels)
----------------------------------------------------------------------
Timings are for 10 derivative evaluations. 
10 points per stencil. COMPACT STENCILS
50^3, INV, 1 func, 1 deriv:  2.4ms on GPU. 
50^3, DiRect (inefficient?), 1 deriv, 2.6ms on GPU

32 points per stencil
50^3, INV, 1 func, 1 deriv:  8.0ms on GPU (twice), 12ms (once). 
50^3, DIRect (inefficient?), 8.5 to 8.7ms on GPU. 
INV should be faster. Not much faster. 

// DOMINATED BY MEMORY, not enough computation? This is the compact case, 
so it is the most efficient. Must repeat with the randomized stencil
distribution. Using 32 points per stencil. 

RANDOMIZED STENCILS
50^3, DIRECT/RANDOM: 16ms
50^3, INVERSE/RANDOM: 17ms (no difference)
----------------------------------------------------------------------
Now run COMPACT/DIRECT, 100^3 grid
DIRECT, GPU: 113ms  (6.6x slower than 50^3, but should only be 8x slower). 
   110ms without optimization
INVERSE, GPU: 130ms  (7.6x slower than 50^3, but should only be 8x slower). 
   Why is INVERSE slower than DIRECT? 
   553 ms without optimization
So Direct is gaining efficiency with larger grids. DO NOT KNOW WHY. 
----------------------------------------------------------------------
INVERSE should be faster in non-optimized for. It isn't. WHY NOT? 
----------------------------------------------------------------------
I tried to speed function up using double4 arguments. This will not work. 
For that, I must return to computing a four derivatives of a single functino. 
----------------------------------------------------------------------
Stencil=32
50^3 grid, 16.9ms (with attribute double4). 16.59 without. Single derivative. 
50^3 grid, 16.9ms (with attribute double4). 16.59 without. Single derivative. 
50^3 grid, 29.4ms (with attribute double4). 28.9 without. Four derivatives.  (double4)
100^3: 140.72 (with attribute vec_hint=double4). Four derivatives. 
100^3: 144.72 (with attribute vec_hint=double3). (not much difference)
CONCLUSION: Since arguments are already double4, there is not much room for improvement. 
100^3: 104.59 (with attribute vec_hint=double4). Single derivative
100^3: 102.00 (with attribute vec_hint=double3). Single derivative.  FUN_KERNEL. 
100^3: 114.09 (with attribute vec_hint=double). Single derivative
100^3: 114.09 (with no attribute vec_hint). Single derivative
100^3: 136.ms (with no attribute vec_hint). Four derivatives, using doubles instead of double4
   (similar to using double4, except that the four derivatives are returned in
   individual arrays.)
   (exact same cost as single derivative, when using doubles. Using double4 is slower.)
CONCLUSION: put an suggestion of double4 being ideal, even if I am using doubles. 10% speed increase on Phi. 

Speed up of 4 derivatives at the same time versus 1 derivative four times:
(102*4) / 142 = 408/142 = 2.8

I have still not experimented with prefetch. 

FINALLY: I should experiment using only doubles, and no double4 arguments.
Will this be more or less efficient? 

printf() works in OpenCL on the MIC. Need a clFinish() to flush the output
buffer. 
----------------------------------------------------------------------
100^3, 110ms, 4 derivatives. 
Removed stencil loop, and computer time went to 2ms. Code was obviously
eliminated. Now remove OpenCL optimization, and run code with and without
stencil loop to establish cost of sending data back to memory. 

Cost without stencil loop: 7.22ms
        derx[i] = dx;    
        dery[i] = dy;    
        derz[i] = dz;    
        derl[i] = dl;    

Cost with stencil loop (non-optimized): 125ms (110ms with optimization)
The difference between optimized and non-optimized code is lessed as we compute more 
derivative since we are improving our use of cache. 

Cost of next 4 lines: 15 ms
        for (int j = 0; j < stencil_size; j++) { 
            int indx = i*stencil_size + j;
            int ind  = indx << 2;
            double sol = solution[stencils[indx]];
		}

Now combine update to memory and solution retrieval: 17ms (should have been 22ms). 
The additional work allowed some of the HW threads to keep busy. 

Now consider only the four weights: 15ms (removing "sol=1" outside the loop
does not change the timing.

       for (int j = 0; j < stencil_size; j++) { 
            int indx = i*stencil_size + j;
            int ind  = indx << 2;
			sol = 1.;
            dx += sol * ww[ind];    
            dy += sol * ww[ind+1];    
            dz += sol * ww[ind+2];    
            dl += sol * ww[ind+3];    
		}

The entire loop without writing derivatives back to memory: 15ms. 

Entire code:  125 ms. 

Entire code, but remove ww[ind], etc...,, but and replacing them by "1.": 88ms. 

HARD TO UNDERSTAND
1 item per group: 130ms
16 items per group (all above results with multi-derivs): 125 ms
64 items per group: 125ms

----------------------------------------------------------------------
void computeDeriv1Weight4Fun1(
         __global int* stencils,
         __global double* ww,
         __global double* solution,
         __global double* derx,
         __global double* dery,
         __global double* derz,
         __global double* derl,
        int nb_stencils,
        int stencil_size)
{
   int i = get_global_id(0);    

   if(i >= nb_stencils) return;

   { 
        double dx = 0.;
        double dy = 0.;
        double dz = 0.;
        double dl = 0.;
        double sol = 1.;

        #if 1
        for (int j = 0; j < stencil_size; j++) { 
            int indx = i*stencil_size + j;
            int ind  = indx << 2;
            #if 1
            double sol = solution[stencils[indx]];
            #else
            #endif


            #if 1
            #if 1
            dx += sol * ww[ind];    
            dy += sol * ww[ind+1];    
            dz += sol * ww[ind+2];    
            dl += sol * ww[ind+3];    
            #else
            dx += sol;
            dy += sol;
            dz += sol;
            dl += sol;
            #endif
            #endif
        }
        #endif

        #if 1
        derx[i] = dx;    
        dery[i] = dy;    
        derz[i] = dz;    
        derl[i] = dl;    
        #endif
   }
   //if (i == 0) printf("exit computeDeriv1Weight4Fun1 CL function\n");
}
----------------------------------------------------------------------

Perhaps, the reason that timings were independent of the size of a workgroup
is because optimization was off. Now we will turn on optimization and redo the 
cases where nb items per workgroup = 1,8,16 and 64. 

Workgroup size, GPU time. 
1, 140ms (was 125 ms unoptimized)
8, 138ms (was 125 ms unoptimized)
32, 138ms (was 125 ms unoptimized)
64, 137ms 
640, 153ms
1024, 158ms
1024 could be the maximum number of threads per workgroup. 

// Computer chooses
>>>> local size: 1000
>>>> number workgroups: 1000
>>>> total nb workitems: 1000000, 
166 ms (optimized code) (chosen by computer, 1000 items/group)
125 ms (non-optimized), 32 items/group 


Do not understand. Unoptimized code is faster.  (125 vs 140 ms) (10%
difference)
----------------------------------------------------------------------
I execute a loop 10 times when I call a derivative kernel. 
I placed a barrier(CLK_GLOBAL_MEM_FENCE) at the end of each loop iteration,
and the time went from 125ms to 116ms (in non-optimized OpenCL). 
There are 32 items per workgroup. 

Return to optimized OpenCL. 
Optimized: same case: 203ms with barriers. 
Optimized: same case: 136ms without barriers. 

BARRIERS VERY EXPENSIVE in optimized mode!!! Unfortunate. Perhaps calling
kernels multiple times is faster, but then one must get data from global
memory multiple times. 

attribute vec_hint: double4
4 deriv * 4 func: 200 ms (optimized code) (RANDOM), 170ms (COMPACT)
(speedup: 2.5x, assuming no errors.)
remove attribute double4, COMPACT: 215ms 
Change to autoselect workgroup size: 210ms
add back attribute double4: 237ms.  (SHOULD BE 170ms)
return to 32 element workgroup size: 204ms (COMPACT) (cannot get back to 170ms) 
RANDOM, 32 el/workgroup: 193ms (same timing as COMPACT)
   WHY? COMPACT USED TO BE FASTER. 
100x50x50: 37ms (COMPACT and RANDOM),  (older timngs were double.)
 (It is possible that my implementation no longer is memory bound?)
stencil: 32
ONE FUNCTION, ONE DERIVATIVE on 100x50x50: 30ms. 
ONE FUNCTION, ONE DERIVATIVE on 100x100x100: 114ms (104ms per derivative)
ONE FUNCTION, FOUR DERIVATIVEs on 100x100x100: 144ms (36ms per derivative, speedup: 2.9)
FOUR FUNCTIONS, FOUR DERIVATIVES on 100x100x100: 208ms (16 derivatives, 13ms per der, speedup: 8) (optimized)
     same case: 216ms (unoptimized compiler option )

functions, 4 derivatives. 100x100x100, 32 nodes per stencil. 
Once I fixed my error in the CPU, time went to 529ms. (from 200ms) :-( for 4
    (same with and without compiler optimization)
Removed hint for double4 packing: time went to 1700ms!! (with compiler optimization off)

Tried 128x128x64 (eliminates potential alignment problems in CL kernel). Time: 620ms (higher GPU time cannot be accounted
for simply by increased grid size.o

611ms: changed CL code to use a temporary. No significant timing difference. 
            double4 www = ww[indx];
            dds0 += sol0 * www; 
            dds1 += sol1 * www;
            dds2 += sol2 * www;
            dds3 += sol3 * www;
            #else
            dds0 += sol0 * ww[indx];
            dds1 += sol1 * ww[indx];
            dds2 += sol2 * ww[indx];
            dds3 += sol3 * ww[indx];
            #endif

I Changed ordering without effect on timing. 
			double sol0 = solution[ind];
            dds0 += sol0 * ww[indx];
			double sol1 = solution[ind +   nb_stencils];
            dds1 += sol1 * ww[indx];
			double sol2 = solution[ind + 2*nb_stencils];
            dds2 += sol2 * ww[indx];
			double sol3 = solution[ind + 3*nb_stencils];
            dds3 += sol3 * ww[indx];
 
I will probably have to implement INV ordering where RBF point if the fastest changing index. 
INV had no effect when I only had a single function. Perhaps efficiency will
increase now because code is more complex to optimzie. 
----------------------------------------------------------------------

DERIVE NOT WORKING (16 deriv)
